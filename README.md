# Retrieval-Augmented Generation (RAG) with LangChain, ChromaDB & Groq

This repository implements an **end-to-end Retrieval-Augmented Generation (RAG)** pipeline for document-based question answering using **LangChain Community**, **Hugging Face embedding models**, **ChromaDB**, and **Groq-hosted LLMs**.

The system retrieves relevant document chunks using semantic search and generates grounded responses based on the retrieved context, improving accuracy and reducing hallucinations.

---

## ğŸš€ Project Overview

The RAG pipeline follows these steps:
1. Load and preprocess source documents  
2. Split documents into manageable text chunks  
3. Generate embeddings using Hugging Face models  
4. Store embeddings in a persistent Chroma vector database  
5. Retrieve relevant context using similarity search  
6. Generate answers using a Groq-powered LLM with source references  

---

## ğŸ§  Architecture

**Documents â†’ Embeddings â†’ Vector Store â†’ Retrieval â†’ LLM â†’ Answer + Sources**

---

## ğŸ› ï¸ Tech Stack

- **Python**
- **LangChain (Community)**
- **Hugging Face ğŸ¤—** â€“ Text embeddings
- **ChromaDB** â€“ Vector database
- **Groq** â€“ Low-latency LLM inference
- **Jupyter Notebook**

---

## ğŸ“‚ Repository Structure

â”œâ”€â”€ data/ # Sample source documents
â”œâ”€â”€ langchain_rag_document_qa.ipynb # Document ingestion & vector creation
â”œâ”€â”€ langchain_rag_retrieval_qa.ipynb # Retrieval + question answering
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ README.md

---

## ğŸ“˜ Notebooks Description

### 1ï¸âƒ£ `langchain_rag_document_qa.ipynb`
- Installs required dependencies  
- Loads and preprocesses documents  
- Splits text into chunks  
- Generates embeddings using Hugging Face models  
- Stores embeddings in a persistent Chroma vector database  

### 2ï¸âƒ£ `langchain_rag_retrieval_qa.ipynb`
- Loads the persisted Chroma vector store  
- Retrieves relevant document chunks using a retriever  
- Uses LangChainâ€™s `RetrievalQA` chain  
- Generates grounded answers using a Groq LLM  
- Returns source document metadata for transparency  

---

## âš™ï¸ Installation

Clone the repository:
```bash
git clone https://github.com/your-username/rag-langchain-groq.git
cd rag-langchain-groq
pip install -r requirements.txt
export GROQ_API_KEY="your_api_key_here"

